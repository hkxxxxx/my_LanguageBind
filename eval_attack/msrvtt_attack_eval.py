import torch
import pandas as pd
import numpy as np
import os
import argparse
from collections import defaultdict
from more_itertools import chunked
from tqdm.auto import tqdm
from autoattack import AutoAttack
from languagebind import LanguageBind, to_device, transform_dict, LanguageBindImageTokenizer
import json
import random
import time

# ========== å‚æ•°é…ç½® (å¯¹é½CLIP RobustBench) ==========
parser = argparse.ArgumentParser(description="MSR-VTTå¯¹æŠ—æ”»å‡»è¯„ä¼°")

# æ¨¡å‹é…ç½®
parser.add_argument('--cache_dir', type=str, default='./cache_dir', help='æ¨¡å‹ç¼“å­˜ç›®å½•')
parser.add_argument('--clip_type', type=str, default='LanguageBind_Video_FT', help='LanguageBindæ¨¡å‹ç±»å‹')

# æ•°æ®é›†é…ç½®  
parser.add_argument('--csv_file', type=str, default='/root/autodl-tmp/LanguageBind/datasets/datasets--friedrichor--MSR-VTT/snapshots/c1af215a96934854f42683c19c51391aaee6f962/raw_data/MSRVTT_JSFUSION_test.csv', help='MSR-VTTæµ‹è¯•é›†CSVæ–‡ä»¶')
parser.add_argument('--video_base_path', type=str, default='/root/autodl-tmp/LanguageBind/datasets/datasets--friedrichor--MSR-VTT/snapshots/c1af215a96934854f42683c19c51391aaee6f962/MSRVTT_Videos/video', help='è§†é¢‘æ–‡ä»¶åŸºç¡€è·¯å¾„')

# æ”»å‡»é…ç½® (å¯¹é½RobustBenchå‚æ•°)
parser.add_argument('--norm', type=str, default='linf', help='æ”»å‡»èŒƒæ•°: linf, l2')
parser.add_argument('--eps', type=float, default=4., help='æ”»å‡»å¼ºåº¦ (0-255èŒƒå›´)')
parser.add_argument('--alpha', type=float, default=2., help='APGD alphaå‚æ•°')
parser.add_argument('--n_samples', type=int, default=200, help='æ¯ç§æ”»å‡»æ–¹æ³•çš„æµ‹è¯•æ ·æœ¬æ•°')
parser.add_argument('--batch_size', type=int, default=16, help='æ‰¹å¤„ç†å¤§å°')

# æ”»å‡»æ–¹æ³•é€‰æ‹©
parser.add_argument('--blackbox_only', type=bool, default=False, help='ä»…è¿è¡Œé»‘ç›’æ”»å‡»')
parser.add_argument('--attacks_to_run', type=str, nargs='+', default=None, 
                   help='æŒ‡å®šè¿è¡Œçš„æ”»å‡»æ–¹æ³•ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰ç™½ç›’æ”»å‡»')

# å®éªŒé…ç½®
parser.add_argument('--experiment_name', type=str, default='msrvtt_attack_eval', help='å®éªŒåç§°')
parser.add_argument('--save_results', type=bool, default=True, help='ä¿å­˜è¯¦ç»†ç»“æœ')
parser.add_argument('--verbose', type=bool, default=True, help='è¯¦ç»†è¾“å‡º')

def compute_metrics(x):
    """è®¡ç®—æ£€ç´¢æŒ‡æ ‡ (å¯¹é½åŸå§‹éªŒè¯è„šæœ¬)"""
    sx = np.sort(-x, axis=1)
    d = np.diag(-x)
    d = d[:, np.newaxis]
    ind = sx - d
    ind = np.where(ind == 0)
    ind = ind[1]
    metrics = {}
    metrics['R1'] = float(np.sum(ind == 0)) * 100 / len(ind)
    metrics['R5'] = float(np.sum(ind < 5)) * 100 / len(ind)
    metrics['R10'] = float(np.sum(ind < 10)) * 100 / len(ind)
    metrics['MR'] = np.median(ind) + 1
    metrics["MedianR"] = metrics['MR']
    metrics["MeanR"] = np.mean(ind) + 1
    return metrics

class LanguageBindAttackWrapper(torch.nn.Module):
    """ç”¨äºAutoAttackçš„æ¨¡å‹åŒ…è£…å™¨ (å¯¹é½CLIP ClassificationModelç»“æ„)"""
    def __init__(self, model, text_embeddings, args):
        super().__init__()
        self.model = model 
        self.text_embeddings = text_embeddings.float()  # [num_texts, embed_dim]
        self.args = args
        
    def forward(self, video_tensor):
        """
        å‰å‘ä¼ æ’­ - è¾“å‡ºlogitsè€Œä¸æ˜¯ç›¸ä¼¼åº¦åˆ†æ•°
        å¯¹é½CLIP RobustBenchçš„ClassificationModel.forward()
        """
        # ç¡®ä¿è¾“å…¥æ˜¯float32
        video_tensor = video_tensor.float()
        
        # è·å–è§†é¢‘embedding
        video_inputs = {'video': {'pixel_values': video_tensor}}
        video_embeddings = self.model(video_inputs)['video']  # [batch_size, embed_dim]
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ [batch_size, num_texts]
        logits = video_embeddings @ self.text_embeddings.T
        
        # å¯¹é½CLIPçš„logit scaling (è™½ç„¶LanguageBindå¯èƒ½æ²¡æœ‰è¿™ä¸ªå‚æ•°)
        # è¿™é‡Œä½¿ç”¨ä¸€ä¸ªå›ºå®šçš„scale factoræ¥å¢å¼ºlogits
        logits = logits * 100  # ç»éªŒæ€§çš„scaling factor
        
        return logits

def load_msrvtt_test_data(csv_file, video_base_path):
    """åŠ è½½MSR-VTTæµ‹è¯•æ•°æ®"""
    df = pd.read_csv(csv_file)
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    valid_indices = []
    for idx, row in df.iterrows():
        video_path = os.path.join(video_base_path, f"{row['video_id']}.mp4")
        if os.path.exists(video_path):
            valid_indices.append(idx)
    
    if len(valid_indices) == 0:
        raise FileNotFoundError(f"åœ¨ {video_base_path} ä¸­æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§†é¢‘æ–‡ä»¶")
    
    df_valid = df.iloc[valid_indices].reset_index(drop=True)
    
    language_data = df_valid['sentence'].values.tolist()
    video_data = df_valid['video_id'].apply(
        lambda x: os.path.join(video_base_path, f'{x}.mp4')
    ).values.tolist()
    
    print(f"æˆåŠŸåŠ è½½ {len(df_valid)} ä¸ªæœ‰æ•ˆçš„è§†é¢‘-æ–‡æœ¬å¯¹")
    return language_data, video_data, df_valid

def get_embeddings(model, tokenizer, language_data, video_data, modality_transform, batch_size):
    """è·å–æ‰€æœ‰æ•°æ®çš„embeddings"""
    print("è®¡ç®—embeddings...")
    
    def embed(x: list[list], dtypes: list[str]) -> dict:
        inputs = {}
        for data, dtype in zip(x, dtypes):
            if dtype == 'language':
                inputs['language'] = to_device(
                    tokenizer(data, max_length=77, padding='max_length', 
                             truncation=True, return_tensors='pt'), device)
            elif dtype in modality_transform:
                inputs[dtype] = to_device(modality_transform[dtype](data), device)
            else:
                raise ValueError(f"Unknown dtype: {dtype}")
        
        with torch.no_grad():
            embeddings = model(inputs)
        
        embeddings = {k: v.detach().cpu().numpy() for k, v in embeddings.items()}
        return embeddings
    
    results = defaultdict(lambda: np.empty((0, 768)))
    
    for batch in tqdm(list(zip(
            chunked(language_data, batch_size),
            chunked(video_data, batch_size)
        )), desc="å¤„ç†æ‰¹æ¬¡"):
        
        embeddings = embed(batch, dtypes=['language', 'video'])
        results['language'] = np.concatenate([results['language'], embeddings['language']])
        results['video'] = np.concatenate([results['video'], embeddings['video']])
    
    return results['video'], results['language']

def compute_accuracy_no_dataloader(model, data, targets, device, batch_size=64):
    """è®¡ç®—å‡†ç¡®ç‡ (å¯¹é½CLIP eval_utilså‡½æ•°)"""
    model.eval()
    correct = 0
    total = 0
    
    with torch.no_grad():
        for i in range(0, len(data), batch_size):
            batch_data = data[i:i+batch_size].to(device)
            batch_targets = targets[i:i+batch_size].to(device)
            
            outputs = model(batch_data)
            _, predicted = torch.max(outputs, 1)
            
            total += batch_targets.size(0)
            correct += (predicted == batch_targets).sum().item()
    
    return correct / total

def evaluate_single_attack(attack_method, model, tokenizer, modality_transform, 
                          language_data, video_data, video_embeddings, text_embeddings, args):
    """è¯„ä¼°å•ä¸ªæ”»å‡»æ–¹æ³• (å¯¹é½RobustBenchè¯„ä¼°æµç¨‹)"""
    print(f"\nè¯„ä¼°æ”»å‡»æ–¹æ³•: {attack_method}")
    
    # éšæœºé€‰æ‹©æµ‹è¯•æ ·æœ¬ - æ³¨æ„è¿™é‡Œåªæ˜¯é€‰æ‹©ç´¢å¼•ï¼Œä¸å½±å“embeddingsè®¡ç®—
    total_samples = len(video_data)
    test_indices = random.sample(range(total_samples), min(args.n_samples, total_samples))
    
    # ä¸ºæ”»å‡»åˆ›å»ºåŒ…è£…æ¨¡å‹
    text_embeddings_tensor = torch.tensor(text_embeddings, dtype=torch.float32).to(device)
    wrapped_model = LanguageBindAttackWrapper(model, text_embeddings_tensor, args)
    
    # å‡†å¤‡æµ‹è¯•æ•°æ® - è¿™é‡Œæ‰æ˜¯çœŸæ­£å½±å“å†…å­˜çš„åœ°æ–¹
    test_videos = []
    test_labels = []
    
    print(f"å‡†å¤‡ {len(test_indices)} ä¸ªæµ‹è¯•æ ·æœ¬...")
    successful_loads = 0
    max_test_samples = min(args.n_samples, 1000)  # é™åˆ¶æœ€å¤§æ ·æœ¬æ•°é¿å…OOM
    
    for test_idx in tqdm(test_indices, desc="åŠ è½½è§†é¢‘"):
        if successful_loads >= max_test_samples:  # è¾¾åˆ°é™åˆ¶å°±åœæ­¢
            break
            
        try:
            video_path = video_data[test_idx]
            if not os.path.exists(video_path):
                continue
                
            # é¢„å¤„ç†è§†é¢‘
            video_tensor_dict = modality_transform['video']([video_path])
            video_tensor_dict = to_device(video_tensor_dict, device)
            video_tensor = video_tensor_dict['pixel_values'].float()  # [1, C, T, H, W]
            
            test_videos.append(video_tensor.squeeze(0))  # ç§»é™¤batchç»´åº¦
            test_labels.append(test_idx)
            successful_loads += 1
            
        except Exception as e:
            if args.verbose:
                print(f"è·³è¿‡æ ·æœ¬ {test_idx}: {str(e)}")
            continue
    
    if len(test_videos) == 0:
        print(f"{attack_method}: æ²¡æœ‰æœ‰æ•ˆçš„æµ‹è¯•æ ·æœ¬")
        return None
    
    # è½¬æ¢ä¸ºtensor
    x_test = torch.stack(test_videos).to(device)
    y_test = torch.tensor(test_labels, dtype=torch.long).to(device)
    
    print(f"å®é™…æµ‹è¯•æ•°æ®å½¢çŠ¶: {x_test.shape}, æ ‡ç­¾æ•°é‡: {len(y_test)}")
    
    # é‡Šæ”¾ä¸å¿…è¦çš„å†…å­˜
    del test_videos
    torch.cuda.empty_cache()
    
    # è®¡ç®—åŸå§‹å‡†ç¡®ç‡ (å¯¹é½RobustBenchæµç¨‹)
    acc = compute_accuracy_no_dataloader(wrapped_model, x_test, y_test, device, min(args.batch_size, 8)) * 100
    print(f'[{attack_method}] åŸå§‹å‡†ç¡®ç‡: {acc:.2f}%', flush=True)
    
    # é…ç½®AutoAttack (å¯¹é½RobustBenchå‚æ•°)
    adversary = AutoAttack(
        wrapped_model, 
        norm=args.norm.replace('l', 'L'),  # è½¬æ¢ä¸ºå¤§å†™ (linf -> Linf)
        eps=args.eps,  # å·²ç»åœ¨mainå‡½æ•°ä¸­è½¬æ¢ä¸º0-1èŒƒå›´
        version='custom',
        attacks_to_run=[attack_method],
        alpha=args.alpha,  # alphaä¹Ÿå·²ç»åœ¨mainå‡½æ•°ä¸­è½¬æ¢
        verbose=args.verbose
    )
    
    # è¿è¡Œæ”»å‡» (å¯¹é½RobustBenchè°ƒç”¨) - ä½¿ç”¨æ›´å°çš„batch size
    start_time = time.time()
    attack_batch_size = min(args.batch_size, 4)  # å‡å°æ”»å‡»æ—¶çš„batch size
    print(f"ä½¿ç”¨æ”»å‡»batch size: {attack_batch_size}")
    
    x_adv, y_adv = adversary.run_standard_evaluation(
        x_test, y_test, bs=attack_batch_size, return_labels=True
    )
    attack_time = time.time() - start_time
    
    # è®¡ç®—é²æ£’å‡†ç¡®ç‡
    racc = compute_accuracy_no_dataloader(wrapped_model, x_adv, y_test, device, min(args.batch_size, 8)) * 100
    
    # è®¡ç®—æ”»å‡»æˆåŠŸç‡
    success_rate = (acc - racc) / acc if acc > 0 else 0
    
    # è®¡ç®—æ£€ç´¢æŒ‡æ ‡ (é¢å¤–çš„MSR-VTTç‰¹å®šæŒ‡æ ‡)
    with torch.no_grad():
        # åŸå§‹ç›¸ä¼¼åº¦
        original_sim = wrapped_model(x_test).cpu().numpy()
        # æ”»å‡»åç›¸ä¼¼åº¦  
        adv_sim = wrapped_model(x_adv).cpu().numpy()
    
    # è®¡ç®—æ£€ç´¢æ€§èƒ½ä¸‹é™
    original_metrics = compute_metrics(original_sim)
    adv_metrics = compute_metrics(adv_sim)
    
    results = {
        'attack_method': attack_method,
        'clean_accuracy': acc,
        'robust_accuracy': racc, 
        'accuracy_drop': acc - racc,
        'success_rate': success_rate,
        'total_samples': len(y_test),
        'attack_time': attack_time,
        'original_r1': original_metrics['R1'],
        'adv_r1': adv_metrics['R1'],
        'r1_drop': original_metrics['R1'] - adv_metrics['R1'],
        'original_r5': original_metrics['R5'],
        'adv_r5': adv_metrics['R5'],
        'r5_drop': original_metrics['R5'] - adv_metrics['R5']
    }
    
    print(f"{attack_method} ç»“æœ:")
    print(f"  åŸå§‹å‡†ç¡®ç‡: {acc:.2f}%")
    print(f"  é²æ£’å‡†ç¡®ç‡: {racc:.2f}%") 
    print(f"  å‡†ç¡®ç‡ä¸‹é™: {acc - racc:.2f}%")
    print(f"  æ”»å‡»æˆåŠŸç‡: {success_rate:.3f}")
    print(f"  R@1: {original_metrics['R1']:.2f}% â†’ {adv_metrics['R1']:.2f}% (ä¸‹é™{original_metrics['R1'] - adv_metrics['R1']:.2f}%)")
    print(f"  æ”»å‡»è€—æ—¶: {attack_time:.2f}ç§’")
    
    # æ¸…ç†å†…å­˜
    del x_test, y_test, x_adv, y_adv, original_sim, adv_sim
    torch.cuda.empty_cache()
    
    return results

def main():
    global device
    
    args = parser.parse_args()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(0)
    np.random.seed(0)
    random.seed(0)
    
    # è½¬æ¢å‚æ•°åˆ°æ­£ç¡®èŒƒå›´ (å¯¹é½RobustBench)
    args.eps = args.eps / 255  # è½¬æ¢ä¸º0-1èŒƒå›´
    args.alpha = args.alpha / 255  # alphaä¹Ÿè½¬æ¢ä¸º0-1èŒƒå›´
    
    print(f"å‚æ•°é…ç½®:\n{'-' * 40}")
    for arg, value in vars(args).items():
        print(f"{arg}: {value}")
    print(f"{'-' * 40}")
    
    # è®¾å¤‡é…ç½®
    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")
    
    # ç¡®å®šæ”»å‡»æ–¹æ³• (å¯¹é½RobustBenché€»è¾‘)
    if args.attacks_to_run is not None:
        attacks_to_run = args.attacks_to_run
    elif args.blackbox_only:
        attacks_to_run = ['square']
    else:
        # æ‰€æœ‰ç™½ç›’æ”»å‡»æ–¹æ³•
        attacks_to_run = ['apgd-ce', 'apgd-dlr', 'apgd-t', 'fab-t']
        # attacks_to_run = ['fab-t']

    print(f'[attacks_to_run] {attacks_to_run}')
    
    # åŠ è½½LanguageBindæ¨¡å‹
    print("åŠ è½½LanguageBindæ¨¡å‹...")
    clip_type = {'video': args.clip_type}
    model = LanguageBind(clip_type=clip_type, cache_dir=args.cache_dir).to(device)
    model.eval()
    
    tokenizer = LanguageBindImageTokenizer.from_pretrained(
        'lb203/LanguageBind_Image', 
        cache_dir=os.path.join(args.cache_dir, 'tokenizer_cache_dir')
    )
    
    modality_transform = {c: transform_dict[c](model.modality_config[c]) for c in clip_type.keys()}
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    language_data, video_data, df_valid = load_msrvtt_test_data(args.csv_file, args.video_base_path)
    
    # è·å–æ‰€æœ‰embeddings
    video_embeddings, text_embeddings = get_embeddings(
        model, tokenizer, language_data, video_data, modality_transform, args.batch_size
    )
    
    # è®¡ç®—åŸºçº¿æ€§èƒ½
    print("\nè®¡ç®—åŸºçº¿æ£€ç´¢æ€§èƒ½...")
    sim_matrix = torch.tensor(video_embeddings @ text_embeddings.T)
    baseline_vt = compute_metrics(sim_matrix)
    baseline_tv = compute_metrics(sim_matrix.T)
    
    print(f"åŸºçº¿æ€§èƒ½ - Video-to-Text: R@1={baseline_vt['R1']:.2f}%, R@5={baseline_vt['R5']:.2f}%, R@10={baseline_vt['R10']:.2f}%")
    print(f"åŸºçº¿æ€§èƒ½ - Text-to-Video: R@1={baseline_tv['R1']:.2f}%, R@5={baseline_tv['R5']:.2f}%, R@10={baseline_tv['R10']:.2f}%")
    
    # å¯¹æ¯ç§æ”»å‡»æ–¹æ³•è¿›è¡Œè¯„ä¼°
    all_results = []
    
    for attack_method in attacks_to_run:
        result = evaluate_single_attack(
            attack_method, model, tokenizer, modality_transform,
            language_data, video_data, video_embeddings, text_embeddings, args
        )
        if result:
            all_results.append(result)
    
    # ========== ç»“æœæ±‡æ€» (å¯¹é½RobustBenchè¾“å‡ºæ ¼å¼) ==========
    print("\n" + "="*100)
    print("æ”»å‡»æ–¹æ³•å¯¹æ¯”ç»“æœæ±‡æ€»:")
    print("="*100)
    print(f"{'æ–¹æ³•':<12} {'åŸå§‹å‡†ç¡®ç‡':<10} {'é²æ£’å‡†ç¡®ç‡':<10} {'å‡†ç¡®ç‡ä¸‹é™':<10} {'R@1ä¸‹é™':<8} {'æ”»å‡»æ—¶é—´':<8}")
    print("-"*100)
    
    # æŒ‰å‡†ç¡®ç‡ä¸‹é™æ’åº (æ”»å‡»æ•ˆæœ)
    all_results.sort(key=lambda x: x['accuracy_drop'], reverse=True)
    
    for result in all_results:
        print(f"{result['attack_method']:<12} "
              f"{result['clean_accuracy']:.2f}%       "
              f"{result['robust_accuracy']:.2f}%       "
              f"{result['accuracy_drop']:.2f}%       "
              f"{result['r1_drop']:.2f}%    "
              f"{result['attack_time']:.1f}s")
    
    # æ‰¾å‡ºæœ€æœ‰æ•ˆçš„æ”»å‡»æ–¹æ³•
    if all_results:
        best_method = all_results[0]
        print(f"\nğŸ† æœ€æœ‰æ•ˆçš„æ”»å‡»æ–¹æ³•: {best_method['attack_method']}")
        print(f"   å‡†ç¡®ç‡ä¸‹é™: {best_method['accuracy_drop']:.2f}%")
        print(f"   R@1ä¸‹é™: {best_method['r1_drop']:.2f}%")
        print(f"   æ”»å‡»æˆåŠŸç‡: {best_method['success_rate']:.3f}")
    
    # ä¿å­˜ç»“æœ (å¯¹é½RobustBenchä¿å­˜æ ¼å¼)
    if args.save_results:
        results_data = {
            'experiment_config': vars(args),
            'baseline_performance': {
                'video_to_text': baseline_vt,
                'text_to_video': baseline_tv
            },
            'attack_results': all_results,
            'summary': {
                'best_attack': best_method['attack_method'] if all_results else None,
                'max_accuracy_drop': best_method['accuracy_drop'] if all_results else 0,
                'max_r1_drop': best_method['r1_drop'] if all_results else 0
            }
        }
        
        timestamp = time.strftime("%Y-%m-%d_%H-%M-%S")
        results_file = f'{args.experiment_name}_{args.norm}_eps{int(args.eps*255)}_{timestamp}.json'
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results_data, f, indent=2, ensure_ascii=False)
        print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {results_file}")

if __name__ == "__main__":
    main()