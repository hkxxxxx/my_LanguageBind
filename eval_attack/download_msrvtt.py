# # 1.å®‰è£…huggingface_hub
# # pip install huggingface_hub
# import os
# from huggingface_hub import snapshot_download
 
# # ä½¿ç”¨cache_dirå‚æ•°ï¼Œå°†æ¨¡å‹/æ•°æ®é›†ä¿å­˜åˆ°æŒ‡å®šâ€œæœ¬åœ°è·¯å¾„â€
# snapshot_download(repo_id="friedrichor/MSR-VTT", repo_type="dataset",
#                   cache_dir="/root/autodl-tmp/LanguageBind/datasets",
#                   local_dir_use_symlinks=False, resume_download=True)

#!/usr/bin/env python3
"""
ç®€åŒ–çš„MSR-VTTæ•°æ®å¤„ç†è„šæœ¬
ç›´æ¥ä½¿ç”¨å·²ä¸‹è½½çš„æ•°æ®
"""

import os
import json
import zipfile
import shutil
from pathlib import Path

def setup_msrvtt_data():
    """å¤„ç†å·²ä¸‹è½½çš„MSR-VTTæ•°æ®"""
    
    # æºè·¯å¾„ï¼ˆä½ ä¸‹è½½çš„æ•°æ®ä½ç½®ï¼‰
    source_dir = Path("/root/autodl-tmp/LanguageBind/datasets/datasets--friedrichor--MSR-VTT/snapshots/c1af215a96934854f42683c19c51391aaee6f962")
    
    # ç›®æ ‡è·¯å¾„ï¼ˆæ•´ç†åçš„æ•°æ®ä½ç½®ï¼‰
    target_dir = Path("/root/autodl-tmp/LanguageBind/datasets/MSR-VTT")
    
    print(f"ğŸ”„ å¤„ç†MSR-VTTæ•°æ®...")
    print(f"æºè·¯å¾„: {source_dir}")
    print(f"ç›®æ ‡è·¯å¾„: {target_dir}")
    
    # åˆ›å»ºç›®æ ‡ç›®å½•
    target_dir.mkdir(parents=True, exist_ok=True)
    videos_dir = target_dir / "videos"
    videos_dir.mkdir(exist_ok=True)
    
    # 1. è§£å‹è§†é¢‘æ–‡ä»¶
    video_zip = source_dir / "MSRVTT_Videos.zip"
    if video_zip.exists():
        print("ğŸ”„ è§£å‹è§†é¢‘æ–‡ä»¶...")
        with zipfile.ZipFile(video_zip, 'r') as zip_ref:
            zip_ref.extractall(videos_dir)
        print(f"âœ… è§†é¢‘æ–‡ä»¶è§£å‹å®Œæˆ")
        
        # æ£€æŸ¥è§£å‹åçš„è§†é¢‘æ•°é‡
        video_count = len(list(videos_dir.rglob("*.mp4")))
        print(f"ğŸ“¹ è§£å‹äº† {video_count} ä¸ªè§†é¢‘æ–‡ä»¶")
    else:
        print("âŒ æœªæ‰¾åˆ°MSRVTT_Videos.zipæ–‡ä»¶")
        return False
    
    # 2. å¤åˆ¶æ ‡æ³¨æ–‡ä»¶
    annotation_files = [
        "msrvtt_train_9k.json",
        "msrvtt_train_7k.json", 
        "msrvtt_test_1k.json"
    ]
    
    print("ğŸ”„ å¤åˆ¶æ ‡æ³¨æ–‡ä»¶...")
    for file_name in annotation_files:
        source_file = source_dir / file_name
        target_file = target_dir / file_name
        if source_file.exists():
            shutil.copy2(source_file, target_file)
            print(f"âœ… å¤åˆ¶: {file_name}")
        else:
            print(f"âš ï¸  æœªæ‰¾åˆ°: {file_name}")
    
    # 3. åˆ›å»ºç»Ÿä¸€çš„æ•°æ®æ–‡ä»¶ï¼ˆåˆå¹¶è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ï¼‰
    create_unified_annotation(target_dir)
    
    print(f"âœ… MSR-VTTæ•°æ®å¤„ç†å®Œæˆ!")
    print(f"ğŸ“ æ•°æ®ä½ç½®: {target_dir}")
    
    return True

def create_unified_annotation(data_dir):
    """åˆ›å»ºç»Ÿä¸€çš„æ ‡æ³¨æ–‡ä»¶"""
    print("ğŸ”„ åˆ›å»ºç»Ÿä¸€æ ‡æ³¨æ–‡ä»¶...")
    
    unified_data = {
        'videos': [],
        'sentences': []
    }
    
    video_id_set = set()
    sentence_id = 0
    
    # å¤„ç†è®­ç»ƒæ•°æ®
    train_file = data_dir / "msrvtt_train_9k.json"
    if train_file.exists():
        with open(train_file, 'r', encoding='utf-8') as f:
            train_data = json.load(f)
        
        # å¤„ç†è§†é¢‘ä¿¡æ¯
        if 'videos' in train_data:
            for video in train_data['videos']:
                video_id = video['video_id']
                if video_id not in video_id_set:
                    unified_data['videos'].append({
                        'video_id': video_id,
                        'url': video.get('url', ''),
                        'start_time': video.get('start time', 0),
                        'end_time': video.get('end time', 0),
                        'split': 'train'
                    })
                    video_id_set.add(video_id)
        
        # å¤„ç†å¥å­ä¿¡æ¯
        if 'sentences' in train_data:
            for sentence in train_data['sentences']:
                unified_data['sentences'].append({
                    'caption': sentence['caption'],
                    'video_id': sentence['video_id'],
                    'sen_id': sentence_id
                })
                sentence_id += 1
    
    # å¤„ç†æµ‹è¯•æ•°æ®
    test_file = data_dir / "msrvtt_test_1k.json"
    if test_file.exists():
        with open(test_file, 'r', encoding='utf-8') as f:
            test_data = json.load(f)
        
        # å¤„ç†è§†é¢‘ä¿¡æ¯
        if 'videos' in test_data:
            for video in test_data['videos']:
                video_id = video['video_id']
                if video_id not in video_id_set:
                    unified_data['videos'].append({
                        'video_id': video_id,
                        'url': video.get('url', ''),
                        'start_time': video.get('start time', 0),
                        'end_time': video.get('end time', 0),
                        'split': 'test'
                    })
                    video_id_set.add(video_id)
        
        # å¤„ç†å¥å­ä¿¡æ¯
        if 'sentences' in test_data:
            for sentence in test_data['sentences']:
                unified_data['sentences'].append({
                    'caption': sentence['caption'],
                    'video_id': sentence['video_id'],
                    'sen_id': sentence_id
                })
                sentence_id += 1
    
    # ä¿å­˜ç»Ÿä¸€çš„æ ‡æ³¨æ–‡ä»¶
    unified_file = data_dir / "MSRVTT_data.json"
    with open(unified_file, 'w', encoding='utf-8') as f:
        json.dump(unified_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»Ÿä¸€æ ‡æ³¨æ–‡ä»¶å·²åˆ›å»º: {unified_file}")
    print(f"ğŸ“Š æ€»è§†é¢‘æ•°: {len(unified_data['videos'])}")
    print(f"ğŸ“Š æ€»å¥å­æ•°: {len(unified_data['sentences'])}")

def verify_data():
    """éªŒè¯æ•°æ®å®Œæ•´æ€§"""
    data_dir = Path("/root/autodl-tmp/LanguageBind/datasets/MSR-VTT")
    
    print("ğŸ” éªŒè¯æ•°æ®å®Œæ•´æ€§...")
    
    # æ£€æŸ¥è§†é¢‘æ–‡ä»¶
    videos_dir = data_dir / "videos"
    video_files = list(videos_dir.rglob("*.mp4"))
    print(f"ğŸ“¹ è§†é¢‘æ–‡ä»¶æ•°é‡: {len(video_files)}")
    
    # æ£€æŸ¥æ ‡æ³¨æ–‡ä»¶
    annotation_file = data_dir / "MSRVTT_data.json"
    if annotation_file.exists():
        with open(annotation_file, 'r', encoding='utf-8') as f:
            data = json.load(f)
        print(f"ğŸ“Š æ ‡æ³¨ä¸­çš„è§†é¢‘æ•°: {len(data['videos'])}")
        print(f"ğŸ“Š æ ‡æ³¨ä¸­çš„å¥å­æ•°: {len(data['sentences'])}")
        
        # æ£€æŸ¥è§†é¢‘æ–‡ä»¶å’Œæ ‡æ³¨çš„åŒ¹é…åº¦
        annotated_videos = set(video['video_id'] for video in data['videos'])
        actual_video_files = set(f.stem for f in video_files)
        
        missing_videos = annotated_videos - actual_video_files
        extra_videos = actual_video_files - annotated_videos
        
        print(f"ğŸ“ˆ åŒ¹é…çš„è§†é¢‘æ•°: {len(annotated_videos & actual_video_files)}")
        if missing_videos:
            print(f"âš ï¸  ç¼ºå¤±çš„è§†é¢‘æ•°: {len(missing_videos)}")
        if extra_videos:
            print(f"â„¹ï¸  é¢å¤–çš„è§†é¢‘æ•°: {len(extra_videos)}")
        
        return True
    else:
        print("âŒ æ ‡æ³¨æ–‡ä»¶ä¸å­˜åœ¨")
        return False

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹å¤„ç†MSR-VTTæ•°æ®...")
    
    # æ£€æŸ¥æºæ•°æ®æ˜¯å¦å­˜åœ¨
    source_dir = Path("/root/autodl-tmp/LanguageBind/datasets/datasets--friedrichor--MSR-VTT/snapshots/c1af215a96934854f42683c19c51391aaee6f962")
    if not source_dir.exists():
        print(f"âŒ æºæ•°æ®ç›®å½•ä¸å­˜åœ¨: {source_dir}")
        print("è¯·ç¡®è®¤æ•°æ®å·²æ­£ç¡®ä¸‹è½½")
        exit(1)
    
    # å¤„ç†æ•°æ®
    if setup_msrvtt_data():
        # éªŒè¯æ•°æ®
        if verify_data():
            print("âœ… MSR-VTTæ•°æ®å¤„ç†æˆåŠŸ!")
            print("\nç°åœ¨å¯ä»¥è¿è¡Œå¯¹æŠ—æ”»å‡»è„šæœ¬äº†:")
            print("python msrvtt_adversarial_attack.py")
        else:
            print("âŒ æ•°æ®éªŒè¯å¤±è´¥")
    else:
        print("âŒ æ•°æ®å¤„ç†å¤±è´¥")